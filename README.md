# Reproduction of MQTransformer

Cite the original paper, which can be found at <https://www.amazon.science/publications/mqtransformer-multi-horizon-forecasts-with-context-dependent-attention-and-optimal-bregman-volatility>

> Chen, K. C., Dicker, L., Eisenach, C., & Madeka, D. (2022). MQTransformer: Multi-Horizon Forecasts with Context Dependent Attention and Optimal Bregman Volatility.

Here is the BibTex for the original paper.

> @Inproceedings{Chen2022,
 author = {Kevin C. Chen and Lee Dicker and Carson Eisenach and Dhruv Madeka},
 title = {MQTransformer: Multi-horizon forecasts with context dependent attention and optimal bregman volatility},
 year = {2022},
 url = {https://www.amazon.science/publications/mqtransformer-multi-horizon-forecasts-with-context-dependent-attention-and-optimal-bregman-volatility},
 booktitle = {KDD 2022 Workshop on Mining and Learning from Time Series – Deep Forecasting: Models, Interpretability, and Applications},
}

Cite this reproduction: TODO

Here is the BibTex for this reproduction: TODO

## Repository Structure 

## Running the Code 

## Miscellaneous 

### Resources 

- ReScience C: <https://rescience.github.io/>
- ReScience C template: <https://github.com/rescience/template>
- ML Reproducibility Tools and Best Practices <https://www.cs.mcgill.ca/~ksinha4/practices_for_reproducibility/>
- Tips for Publishing Research Code: <https://github.com/paperswithcode/releasing-research-code>
- Learn To Reproduce Papers: Beginner’s Guide <https://towardsdatascience.com/learn-to-reproduce-papers-beginners-guide-2b4bff8fcca0>
- Common Problems When Reproducing A Machine Learning Paper <https://derekchia.com/common-problems-when-reproducing-a-machine-learning-paper/>
- The Machine Learning Reproducibility Checklist: <https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf>
- Learn To Reproduce Papers: Beginner’s Guide: <https://towardsdatascience.com/learn-to-reproduce-papers-beginners-guide-2b4bff8fcca0>
- Quantifying Independently Reproducible Machine Learning: <https://thegradient.pub/independently-reproducible-machine-learning/>
- How to Solve Reproducibility in ML: <https://neptune.ai/blog/how-to-solve-reproducibility-in-ml>
- Reproducibility Resources: <https://www.cs.mcgill.ca/~jpineau/ICLR2018-ReproducibilityChallenge-Readings.pdf>
- ML Reproducibility Challenge 2021: <https://paperswithcode.com/rc2021>
- Papers with Code: <https://paperswithcode.com/>
- Code that helped me: 
    - <https://keras.io/examples/timeseries/timeseries_traffic_forecasting/>
    - <https://keras.io/examples/timeseries/timeseries_classification_transformer/>
- Related code bases: <https://paperswithcode.com/search?q_meta=&q_type=&q=transformer+time+series>
    - Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting: <https://paperswithcode.com/paper/temporal-fusion-transformers-for> and <https://github.com/google-research/google-research/tree/master/tft> 
    - PyTorch forecasting: <https://github.com/jdb78/pytorch-forecasting>
    - Are Transformers Effective for Time Series Forecasting?: <https://paperswithcode.com/paper/are-transformers-effective-for-time-series> and <https://github.com/cure-lab/DLinear>
    - TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data <https://paperswithcode.com/paper/tranad-deep-transformer-networks-for-anomaly> and <https://github.com/imperial-qore/tranad>
    - NAST: Non-Autoregressive Spatial-Temporal Transformer for Time Series Forecasting <https://paperswithcode.com/paper/nast-non-autoregressive-spatial-temporal> and <https://github.com/Flawless1202/Non-AR-Spatial-Temporal-Transformer>
    - Transformers in Time Series: A Survey <https://paperswithcode.com/paper/transformers-in-time-series-a-survey> and <https://github.com/qingsongedu/time-series-transformers-review>
    - (*) Time-series Transformer Generative Adversarial Networks <https://paperswithcode.com/paper/time-series-transformer-generative> and <https://github.com/jsyoon0823/TimeGAN>